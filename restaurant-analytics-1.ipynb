{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd21a08-81ac-42cb-afb0-1a3e8474f37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Yasaman Emami\"\n",
    "__email__ = ['emami.yasamann@gmail.com','yasaman.emami@sjsu.edu']\n",
    "__sid__ = \"015325557\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9f767f-501f-4997-980b-9bf35e4f3bca",
   "metadata": {},
   "source": [
    "## Reading data files as tsv and union all to make one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53789599-80a0-4d0d-ac5f-3238591f132f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/yasamanemami/Documents/DATA-228%20Sec%2022%20-%20Big%20Data%20Tech%20and%20App/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/22 22:37:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/22 22:38:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+---------------+-------------+---+-----+---------+-----------+--------------------+-----------+-------------+-------------------+----+--------------------+--------------------+--------------------+--------+\n",
      "| _c0|                 _c1|            _c2|          _c3|_c4|  _c5|      _c6|        _c7|                 _c8|        _c9|         _c10|               _c11|_c12|                _c13|                _c14|                _c15|    _c16|\n",
      "+----+--------------------+---------------+-------------+---+-----+---------+-----------+--------------------+-----------+-------------+-------------------+----+--------------------+--------------------+--------------------+--------+\n",
      "|2764|            AL-HAMRA|   3083 16th St|San Francisco| CA|94103|37.764913|-122.421349|{'needs_recoding'...|       null|2764_20160106|2016-01-06T00:00:00|null|           Complaint|                null|                null|    null|\n",
      "|1154|SUNFLOWER RESTAURANT|506 Valencia St|San Francisco| CA|94103|37.764678|-122.421905|{'needs_recoding'...|14155625023|1154_20160105|2016-01-05T00:00:00|  88|Routine - Unsched...|1154_20160105_103149|Wiping cloths not...|Low Risk|\n",
      "+----+--------------------+---------------+-------------+---+-----+---------+-----------+--------------------+-----------+-------------+-------------------+----+--------------------+--------------------+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "  \n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "data16 = spark.read.csv(\"dataset2016.tsv\", sep=r'\\t', header=False)\n",
    "data17 = spark.read.csv(\"dataset2017.tsv\", sep=r'\\t', header=False)\n",
    "data18 = spark.read.csv(\"dataset2018.tsv\", sep=r'\\t', header=False)\n",
    "data = data16.union(data17).union(data18)\n",
    "\n",
    "data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "076e88c8-3727-41b8-9050-a2c7c1f70048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51731"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b67bf27e-b47b-43dd-96d0-706ee77e91fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the name of columns to names related to the values\n",
    "data_with_header = data.selectExpr(\"_c0 as restaurant_id\", \"_c1 as restaurant_name\",\"_c2 as restaurant_address\",\n",
    "                                \"_c3 as restaurant_city\",\"_c4 as restaurant_state\",\"_c5 as restaurant_zipcode\",\n",
    "                                \"_c6 as latitude\",\"_c7 as longitude\",\n",
    "                                \"_c8 as record_dict\",\"_c9 as number\",\"_c10 as id_start_date\",\"_c11 as start_date\",\n",
    "                                \"_c12 as rating\",\"_c13 as schedule\",\"_c14 as second_id\",\"_c15 as required_inspect\",\n",
    "                                \"_c16 as risk_level\")\n",
    "#data_col_name.schema.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1fbdc5-2b16-4b8f-a9f2-620db02d0379",
   "metadata": {},
   "source": [
    "## show descriptive statistics for numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af0e63b2-47de-48fd-bb5d-61ba4db4635f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|          latitude|\n",
      "+-------+------------------+\n",
      "|  count|             29189|\n",
      "|   mean|37.743478438658606|\n",
      "| stddev|1.0368811059679102|\n",
      "|    min|                 0|\n",
      "|    max|         37.824494|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_with_header.describe(\"latitude\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a93a773-2103-4fce-a5a3-75edda1a92f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|          longitude|\n",
      "+-------+-------------------+\n",
      "|  count|              29189|\n",
      "|   mean|-122.33581876076603|\n",
      "| stddev|  3.360015099603407|\n",
      "|    min|        -122.368257|\n",
      "|    max|                  0|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_with_header.describe(\"longitude\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296b682a-5441-4e88-9b3f-acb341bb035a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|              number|\n",
      "+-------+--------------------+\n",
      "|  count|               15967|\n",
      "|   mean|1.415540227462923...|\n",
      "| stddev|  1226179.3764345855|\n",
      "|    min|         14150204876|\n",
      "|    max|         14159881393|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_with_header.describe(\"number\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bab7bc7a-c675-419e-9b76-43b5aeb53032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|           rating|\n",
      "+-------+-----------------+\n",
      "|  count|            38200|\n",
      "|   mean|85.94552356020942|\n",
      "| stddev|8.780569238373296|\n",
      "|    min|              100|\n",
      "|    max|               98|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_with_header.describe(\"rating\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78c2581f-b748-492d-83a8-e5aa1e19e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new df from numerical column values\n",
    "df_numerical_cols = data_with_header.selectExpr(\"cast(restaurant_id as int) restaurant_id\",\n",
    "                                                \"cast(restaurant_zipcode as int) restaurant_zipcode\",\n",
    "                                                \"cast(latitude as float) latitude\",\n",
    "                                                \"cast(longitude as float) longitude\",\n",
    "                                                \"cast(number as bigint) number\",\n",
    "                                                \"cast(rating as float) rating\",\n",
    "                                               )\n",
    "\n",
    "#df_numerical_cols.printSchema()\n",
    "#df_numerical_cols.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed8eada-b8df-47dc-b14a-09c6f5de4892",
   "metadata": {},
   "source": [
    "## drop duplicates and fill null values for numerical columns with mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b51a14a-20f1-484c-adb6-d2eac7e06f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "#drop duplicate rows in df\n",
    "data_without_duplicates = data_with_header.dropDuplicates()\n",
    "\n",
    "#data_without_duplicates.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15f55ce4-8272-44a9-be21-d9b2b8b59e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+--------+---------+------+------+\n",
      "|restaurant_id|restaurant_zipcode|latitude|longitude|number|rating|\n",
      "+-------------+------------------+--------+---------+------+------+\n",
      "|            0|              1256|   22542|    22542| 35764| 13531|\n",
      "+-------------+------------------+--------+---------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#showing null counts in each column\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df_numerical_cols.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_numerical_cols.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cee3c895-14b1-4784-adb9-5b1baad070db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing null zipcode with the avg value\n",
    "zipcode_mean = df_numerical_cols.agg({'restaurant_zipcode': 'mean'}).take(1)[0][0]\n",
    "df1 = df_numerical_cols.na.fill(zipcode_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36ecda28-126d-4c83-b3a6-06a9720aa6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing null latitude with the avg value\n",
    "latitude_mean = df1.agg({'latitude': 'mean'}).take(1)[0][0]\n",
    "df2 = df1.na.fill(latitude_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de86704c-da25-4f70-ab13-542d78cf4f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing null longitude with the avg value\n",
    "longitude_mean = df2.agg({'longitude': 'mean'}).take(1)[0][0]\n",
    "df3 = df2.na.fill(longitude_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "401b36db-6936-4843-8009-d7499799fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing null number with the avg value\n",
    "number_mean = df3.agg({'number': 'mean'}).take(1)[0][0]\n",
    "df4 = df3.na.fill(number_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53be4c90-6a03-456c-adf2-55145e0e02c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#replacing null rating with the avg value\n",
    "rating_mean = df4.agg({'rating': 'mean'}).take(1)[0][0]\n",
    "df5 = df4.na.fill(rating_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9c305d3-15fe-4bd0-9760-944a693f7120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+--------+---------+------+------+\n",
      "|restaurant_id|restaurant_zipcode|latitude|longitude|number|rating|\n",
      "+-------------+------------------+--------+---------+------+------+\n",
      "|            0|                 0|       0|        0|     0|     0|\n",
      "+-------------+------------------+--------+---------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#checking if the null values has been filled\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "\n",
    "df5.select(*(count(when(col(c).isNull(), c)).alias(c) for c in df5.columns)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da70f4d2-abab-448f-9bc0-ff01bbf079c3",
   "metadata": {},
   "source": [
    "## display correlation matrix in two different ways"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c25f0b1b-06dc-46a6-9fc0-9454ae375218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, -0.01948022584120647, -0.7971770861712276, 0.7971902932030105, 0.017755279693359438, -0.12813656738625975, -0.01948022584120647, 1.0, 0.0006860162285308216, -0.0006890228463521726, 0.009037958486648222, -0.0007724602081661724, -0.7971770861712277, 0.0006860162285308205, 1.0, -0.999999393525567, -0.1273698092054171, 0.13218872540238097, 0.7971902932030105, -0.0006890228463521711, -0.999999393525567, 1.0, 0.1271932569380599, -0.13218753277124973, 0.01775527969335945, 0.009037958486648219, -0.12736980920541716, 0.1271932569380599, 1.0, -0.013449288746996869, -0.12813656738625975, -0.0007724602081661749, 0.13218872540238097, -0.1321875327712497, -0.013449288746996885, 1.0]\n"
     ]
    }
   ],
   "source": [
    "#finding correlation matrix approach 1\n",
    "corr_matrix =[]\n",
    "\n",
    "df_num_len = len(df_numerical_cols.columns)\n",
    "\n",
    "for i in df_numerical_cols.columns:\n",
    "    for j in df_numerical_cols.columns:\n",
    "        corr_matrix.append(df_numerical_cols.corr(i,j,method=\"pearson\"))\n",
    "\n",
    "print(corr_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a35ebed2-b9f2-492d-904c-23a0d625d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/13 15:17:18 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/10/13 15:17:18 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|pearson(corr_features)                                                                                                                                                                                                                                                                                               |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1.0                   -0.01919108505535854    ... (6 total)\n",
      "-0.01919108505535854  1.0                     ...\n",
      "0.7968441185428977    -3.8107213235758885E-4  ...\n",
      "0.7968446385134561    -3.8107066893489154E-4  ...\n",
      "0.01775527956168689   0.009095855536852946    ...\n",
      "0.12842966278870205   0.002518507833322687    ...|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#finding corr matrix approach2\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# convert to vector column first\n",
    "vector_col = \"corr_features\"\n",
    "assembler = VectorAssembler(inputCols=df5.columns, outputCol=vector_col)\n",
    "df_vector = assembler.transform(df5).select(vector_col)\n",
    "\n",
    "# get correlation matrix\n",
    "matrix = Correlation.corr(df_vector, vector_col)\n",
    "matrix.show(truncate =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6eb15025-c9af-43b3-9b8d-91936d727fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data_zipcode_query = data_with_header.selectExpr(\"cast(restaurant_id as int) restaurant_id\",\n",
    "                                                        \"cast(restaurant_zipcode as int) restaurant_zipcode\",\n",
    "                                                        \"cast(latitude as float) latitude\",\n",
    "                                                        \"cast(longitude as float) longitude\",\n",
    "                                                        \"cast(number as bigint) number\",\n",
    "                                                        \"cast(rating as float) rating\",\n",
    "                                                        \"cast(restaurant_name as string) restaurant_name\")\n",
    "orig_df1 = clean_data_zipcode_query.na.fill(zipcode_mean)\n",
    "orig_df2 = orig_df1.na.fill(latitude_mean)\n",
    "orig_df3 = orig_df2.na.fill(longitude_mean)\n",
    "orig_df4 = orig_df3.na.fill(number_mean)\n",
    "orig_df5 = orig_df4.na.fill(rating_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141a132-bf31-482a-ae8b-2b05cb70f6cc",
   "metadata": {},
   "source": [
    "## showing number of distinct restaurants in each zipcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "beade666-f8d1-47c0-88aa-000d9955c5a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:==========================================>          (160 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------------------------+\n",
      "|restaurant_zipcode|count(DISTINCT restaurant_name)|\n",
      "+------------------+-------------------------------+\n",
      "|             94621|                              1|\n",
      "|             94109|                            377|\n",
      "|             94402|                              1|\n",
      "|             94901|                              1|\n",
      "|             94115|                            222|\n",
      "|             95122|                              1|\n",
      "|             94112|                            189|\n",
      "|             94127|                             68|\n",
      "|             94013|                              2|\n",
      "|             94108|                            215|\n",
      "|             94121|                            150|\n",
      "|             94101|                              2|\n",
      "|             94105|                            211|\n",
      "|             94120|                              1|\n",
      "|             94131|                             49|\n",
      "|             92672|                              1|\n",
      "|             94116|                             93|\n",
      "|             94134|                             78|\n",
      "|             94014|                              2|\n",
      "|             95133|                              1|\n",
      "+------------------+-------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sprk = SparkSession.builder.getOrCreate()\n",
    "\n",
    "orig_df5.createOrReplaceTempView(\"final\")\n",
    "sqlCount3 = sprk.sql(\"SELECT  restaurant_zipcode, count(distinct restaurant_name) FROM final where restaurant_zipcode<99999 group by restaurant_zipcode\")\n",
    "sqlCount3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3044e5c8-138e-4e2c-9a11-b24bcb37eb78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
